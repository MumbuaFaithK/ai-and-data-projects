{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWQeg7MFBSWUMKco1f+NXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MumbuaFaithK/ai-and-data-projects/blob/main/Faith_Mumbua_BERT_Sentence_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Similarity with BERT\n",
        "## Faith Mumbua | Natural Language Processing"
      ],
      "metadata": {
        "id": "QIQy-yeBCzCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "*   In this assignment, i use a pre-trained BERT model from Hugging Face to compute sentence similarity\n",
        "*   using contextual embeddings. i'll extract [CLS] token embeddings for sentence pairs, calculate cosine\n",
        "\n",
        "\n",
        "*   similarity scores, predict similarity based on a threshold, and evaluate the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GI8GIgMyGVYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install and import required libraries"
      ],
      "metadata": {
        "id": "mYPxLffvIdwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twxqMevoIfjV",
        "outputId": "82dbcde6-ea96-45c7-e077-3127861038fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load pre-trained BERT tokenizer and model"
      ],
      "metadata": {
        "id": "tCPBu_1IJcoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRGHaMNNJi8i",
        "outputId": "754d3f84-ab3c-4801-956f-863f9f6f48aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define sentence pairs and labels\n"
      ],
      "metadata": {
        "id": "MXZlEocSK_ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 given + 5 new sentence pairs\n",
        "sentence_pairs = [\n",
        "    (\"How do I learn Python?\", \"What is the best way to study Python?\"),\n",
        "    (\"What is AI?\", \"How to cook pasta?\"),\n",
        "    (\"How do I bake a chocolate cake?\", \"Give me a chocolate cake recipe.\"),\n",
        "    (\"How can I improve my coding skills?\", \"Tips for becoming better at programming.\"),\n",
        "    (\"Where can I buy cheap laptops?\", \"Best sites to find affordable computers.\"),\n",
        "    # Additional 5 sentence pairs\n",
        "    (\"I love playing football.\", \"Soccer is my favorite sport.\"),\n",
        "    (\"The weather is sunny today.\", \"It is raining heavily in Nairobi.\"),\n",
        "    (\"Can I travel to Mombasa without an ID?\", \"Do I need identification to travel to Mombasa?\"),\n",
        "    (\"She is reading a novel.\", \"He is watching a movie.\"),\n",
        "    (\"My phone battery dies quickly.\", \"My smartphone doesn’t last long on charge.\"),\n",
        "]\n",
        "\n",
        "# Ground truth similarity labels\n",
        "labels = [1, 0, 1, 1, 1, 1, 0, 1, 0, 1]"
      ],
      "metadata": {
        "id": "LeH2EX9RMfFW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Define function to get [CLS] embedding for a sentence"
      ],
      "metadata": {
        "id": "uZdLCzCpMr0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embedding(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='tf', padding=True, truncation=True)\n",
        "    outputs = bert_model(inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "    return cls_embedding.numpy()"
      ],
      "metadata": {
        "id": "f6-2r1xLMxQA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Calculate cosine similarity for each pair and predict similarity"
      ],
      "metadata": {
        "id": "WT_DCj85M435"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for sent1, sent2 in sentence_pairs:\n",
        "    emb1 = get_sentence_embedding(sent1)\n",
        "    emb2 = get_sentence_embedding(sent2)\n",
        "    sim_score = cosine_similarity(emb1, emb2)[0][0]\n",
        "    pred = 1 if sim_score > 0.7 else 0\n",
        "    predictions.append(pred)\n",
        "\n",
        "    print(f\"\\nSentence 1: {sent1}\")\n",
        "    print(f\"Sentence 2: {sent2}\")\n",
        "    print(f\"Cosine Similarity: {sim_score:.4f} → Predicted Similar: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbNEnXxxM_Fb",
        "outputId": "9b7531c0-bb9a-487a-e0b3-6bdeec842323"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1: How do I learn Python?\n",
            "Sentence 2: What is the best way to study Python?\n",
            "Cosine Similarity: 0.9743 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: What is AI?\n",
            "Sentence 2: How to cook pasta?\n",
            "Cosine Similarity: 0.9033 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: How do I bake a chocolate cake?\n",
            "Sentence 2: Give me a chocolate cake recipe.\n",
            "Cosine Similarity: 0.8938 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: How can I improve my coding skills?\n",
            "Sentence 2: Tips for becoming better at programming.\n",
            "Cosine Similarity: 0.8633 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: Where can I buy cheap laptops?\n",
            "Sentence 2: Best sites to find affordable computers.\n",
            "Cosine Similarity: 0.8750 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: I love playing football.\n",
            "Sentence 2: Soccer is my favorite sport.\n",
            "Cosine Similarity: 0.9594 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: The weather is sunny today.\n",
            "Sentence 2: It is raining heavily in Nairobi.\n",
            "Cosine Similarity: 0.8443 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: Can I travel to Mombasa without an ID?\n",
            "Sentence 2: Do I need identification to travel to Mombasa?\n",
            "Cosine Similarity: 0.9794 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: She is reading a novel.\n",
            "Sentence 2: He is watching a movie.\n",
            "Cosine Similarity: 0.9193 → Predicted Similar: 1\n",
            "\n",
            "Sentence 1: My phone battery dies quickly.\n",
            "Sentence 2: My smartphone doesn’t last long on charge.\n",
            "Cosine Similarity: 0.9145 → Predicted Similar: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluate accuracy"
      ],
      "metadata": {
        "id": "E22dxBlxNNJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = sum([1 for i in range(len(labels)) if predictions[i] == labels[i]])\n",
        "accuracy = correct / len(labels)\n",
        "print(f\"\\nAccuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35PH2SHNTbK",
        "outputId": "2d7dd3ac-e870-4ee6-a257-2d285409d8f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 70.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions"
      ],
      "metadata": {
        "id": "u_wB9ZChNZsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  How does BERT differ from traditional NLP approaches like Bag of Words or TF-IDF?\n",
        "### Traditional models like BoW or TF-IDF assign static vectors to words, ignoring word order and context.\n",
        "### BERT generates contextual embeddings where the same word can have different meanings depending on surrounding words.\n",
        "\n",
        "# What is the role of the encoder in the BERT model?\n",
        "### BERT uses a stack of Transformer encoders to learn relationships between words. In this assignment,\n",
        "### use the encoder's output (specifically the [CLS] token) to represent the sentence.\n",
        "\n",
        "# What are contextual embeddings?\n",
        "### Contextual embeddings are vector representations of words based on their usage in context.\n",
        "### BERT creates them dynamically for each token based on the entire sentence. This allows it to capture nuances in meaning.\n",
        "\n",
        "# Why is the [CLS] token used for sentence similarity?\n",
        "### The [CLS] token is a special token added to the beginning of each sentence.\n",
        " ### BERT is trained to use the [CLS] output to summarize the sentence. We use it to represent the whole sentence when computing similarity.\n",
        "\n",
        "# What is cosine similarity and why is it useful?\n",
        "### Cosine similarity measures the angle between two vectors and indicates how similar they are regardless of magnitude.\n",
        "### It is effective for comparing high-dimensional embeddings like those from BERT.\n"
      ],
      "metadata": {
        "id": "JOW2mtoPNein"
      }
    }
  ]
}